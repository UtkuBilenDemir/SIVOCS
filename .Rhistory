#--------------- CUSTOM FUNCTIONS
# Correlation matrix with lm model
corr_matrix_plt <- function (df, colcol=" ") {pairs.panels(
x=df,
method = "spearman", # correlation method
hist.col = ifelse(colcol==" ",  "#00AFBB", colcol),
density = TRUE,  # show density plots
ellipses = TRUE, # show correlation ellipses
lm = TRUE,
stars=TRUE,
jiggle=TRUE,
factor=2
)
}
# MinMaxScaler from Python
normalize <- function(x, na.rm = TRUE) {
return((x- min(x)) /(max(x)-min(x)))
}
cat(paste0("99 sourced"))
cor_mat <- cor(data.questions[,as.vector(sapply(data.questions, FUN=is.numeric))],  use = "complete.obs", method="spearman")
write.csv(cor_mat, "cor_mat.csv")
#--- Scatter plot
lm_scatter <- function(df, x, y, labx="", laby="", title="", posx=9, posy=0) {
ggplotly(
ggplot(df,
aes(y = y, x = x)) +
geom_point() +
ggtitle(title) +
geom_smooth(method=lm, se=TRUE, fullrange=TRUE, color='#e7298a')+
geom_jitter(color='#377eb8')+
theme_minimal() +
geom_point(color='#377eb8') +
labs(
y=laby,
x =labx) +
stat_cor(label.x = posx, label.y =posy , cor.coef.name = "rho", output.type = "text", method = "spearman") +
stat_regline_equation(output.type = "text",label.x = posx, label.y = posy +1)
)
}
# A custom function to recode numerical responses into ordered factors
likert_recode <- function(x) {
y <- ifelse(is.na(x), NA,
ifelse(x <= 3, "0...not at all - 3",
ifelse(x > 3 & x <7, "4 - 6" ,
ifelse(x >= 7, "7 - 10...fully", " "))))
y <- factor(y, levels = c("0...not at all - 3", "4 - 6", "7 - 10...fully"))
return(y)
}
likert_recode2 <- function(x) {
y <- ifelse(is.na(x), NA,
ifelse(x == 0, "No",
ifelse(x == 1, "Only Marginally" ,
ifelse(x == 2, "Quite Centrally", " "))))
y <- factor(y, levels = c("No", "Only Marginally" , "Quite Centrally"))
return(y)
}
likert_recode3 <- function(x) {
y <- ifelse(is.na(x), NA,
ifelse(x == 0, "No Involvement",
ifelse(x == 1, "Consultative" ,
ifelse(x == 2, "Contributory",
ifelse(x == 3, "Collaboratively",
ifelse(x == 4, "Co-created", ""))))))
y <- factor(y, levels = c("No Involvement",
"Consultative" ,
"Contributory",
"Collaboratively",
"Co-created"
))
return(y)
}
likert_recode4 <- function(x) {
y <- ifelse(is.na(x), NA,
ifelse(x == 0, NA,
ifelse(x == 1, "Consultative" ,
ifelse(x == 2, "Contributory",
ifelse(x == 3, "Collaboratively",
ifelse(x == 4, "Co-created", ""))))))
y <- factor(y, levels = c(
"Consultative" ,
"Contributory",
"Collaboratively",
"Co-created"
))
return(y)
}
#--- A standardized plot for likert variables
likert_plot <- function(recoded_likert, h=TRUE, t=4, c = 1:3) {
plot( recoded_likert,
# Group the items alphabetically
# group.order=names(colnames(e12.trans_df)),
# Plot the percentages for each response category
plot.percents = TRUE,
# Plot the total percentage for negative responses
plot.percent.low = FALSE,
# Plot the total percentage for positive responses
plot.percent.high = FALSE,
# Whether response categories should be centered
# This is only helpful when there is a middle response
# option such as "neutral" or "neither agree nor disagree"
centered = TRUE,
include.histogram = h,
# Wrap label text for item labels
wrap=80,
text.size=t,
colors =  c("#D32F49", "#F4A582", "#DFDFDF", "#92C5DE", "#338BBE")[c]
)
}
#--- This is good
# install.packages("GGally")
library(GGally)
e12_disc <- na.omit(as.data.frame(cbind(e12.trans_df, domain=data$domain)))
e12_disc[, 1:3] <- sapply(FUN=as.numeric,e12_disc[, 1:3])
ggpairs(e12_disc, columns = 1:3, aes(color=domain))
e12_disc[,1]
rm(list = ls())
library(ggplot2)
library(ggbiplot)
library(psych)
# Data Frame
source("./02_analysis/02_static_responses.R")
# colnames of the specific question groups
source("./02_analysis/99_question_groups.R")
# -------------------- PCA
# Are there completely NA colums?
sum(apply(FUN = sum,
MARGIN = 2,
apply(FUN = is.na,
MARGIN = 2,
data.num_questions)) == nrow(data.num_questions
)
)
pca_model <- prcomp(na.omit(data.num_questions),
scale = TRUE,
center = TRUE)
#plot(pca_model$x[, 1], pca_model$x[, 2])
# How much variation in each component
pca_model.var <- pca_model$sdev^2
pca_model.var_per <- cumsum(pca_model.var)/sum(pca_model.var)
barplot(pca_model.var_per,
main = "Scree Plot")
# A more meaningful visualisation of PCA
pca_model.data <- data.frame(Sample = row.names(pca_model$x),
X = pca_model$x[, 1],
Y = pca_model$x[, 2]
)
ggplot(data = pca_model.data,
aes(x = X, y = Y, label = Sample)) +
geom_text() +
xlab(paste("PC1 - ", round(pca_model.var_per[1], 2), "%", sep = "")) +
ylab(paste("PC2 - ", round(pca_model.var_per[2], 2), "%", sep = "")) +
theme_bw() +
ggtitle("First 2 components")
# Visualisation of all components
ggbiplot(pca_model)
# Most important features
loading_scores <- abs(pca_model$rotation[, 1])
loading_scores.ranked <- sort(loading_scores, decreasing = TRUE)
top_10_features <- loading_scores.ranked[1:10]
top_10_features
# -------------------- Factor Analysis
parallel <- fa.parallel(data.num_questions,
fm = "minres",
fa = 'fa')
cumsum(parallel)
factors <- fa(data.num_questions,
nfactors = 10,
rotate = 'oblimin',
fm = 'minres')
print(factors)
library(lavaan)
print(factors)
factors$factors
factors$fit
factors$fit.off
factors$loadings
str(factors$loadings)
colnames(factors$loadings)
factors$loadings[, "MR3"]
factors$loadings[, "MR3"] > 0.4
names(factors$loadings[, "MR3"] > 0.4)
rownames(factor$loadings)
factor$loadings[1] (factors$loadings[, "MR3"] > 0.4
factor$loadings[,1]
head(factor$loadings) (factors$loadings[, "MR3"] > 0.4
head(factor$loadings)
as.data.frame(factor$loadings)
str(factor$loadings)
head(factors$loadings)
rownames(factors$loadings)
rownames(factors$loadings)[ (factors$loadings[, "MR3"] > 0.4]
as.vector(rownames(factors$loadings))[(factors$loadings[, "MR3"] > 0.4]
as.vector(rownames(factors$loadings))[(factors$loadings[, "MR3"] > 0.4)]
as.vector(rownames(factors$loadings))[(factors$loadings[, "MR4"] > 0.4)]
as.vector(rownames(factors$loadings))[(factors$loadings[, "MR5"] > 0.4)]
as.vector(rownames(factors$loadings))[(factors$loadings[, "MR6"] > 0.4)]
as.vector(rownames(factors$loadings))[(factors$loadings[, "MR7"] > 0.4)]
as.vector(rownames(factors$loadings))[(factors$loadings[, "MR8"] > 0.4)]
as.vector(rownames(factors$loadings))[(factors$loadings[, "MR9"] > 0.4)]
as.vector(rownames(factors$loadings))[(factors$loadings[, "MR10"] > 0.4)]
model <- '
F1 =~ groupsInvolved.civsoc.+groupsInvolved.citiz.+groupsInvolved.welfare.+targetGroupsGoals.socneeds.+targetGroupsGoals.socgroups.+targetGroupsGoals.empower.
F2 =~ adoptByPolicy.rate.+Impactstatements.capab.+Impactstatements.emanc.+Impactstatements.understanding.+Impactstatements.mitig.+Impactstatements.unknown.+Impactstatements.unaddressed.
F3 =~ concepts.pub.+concepts.data.+concepts.code.+concepts.infra.+dissChannels.trad.+dissChannels.web.+dissChannels.platf.
F4 =~ argetGroupsGoals.diversity.
F5 =~ scalabilityRating.up.+scalabilityRating.out.+scalabilityRating.deep.
F6 =~ groupsInvolved.policy.+impactTargetGroup.policy.+dissChannels.policy.
F7 =~ transdisciplinaryExp.rate.+groupsInvolved.busi.+impactTargetGroup.pub.+impactTargetGroup.busi.
F8 =~ dissChannels.conf.
F9 =~ contribToSI.rate.
F10 =~ motivation.pheno.+motivation.prob.
'
fit <- cfa(model, data = data.num_questions)
model <- '
F1 =~ groupsInvolved.civsoc.+groupsInvolved.citiz.+groupsInvolved.welfare.+targetGroupsGoals.socneeds.+targetGroupsGoals.socgroups.+targetGroupsGoals.empower.
F2 =~ adoptByPolicy.rate.+Impactstatements.capab.+Impactstatements.emanc.+Impactstatements.understanding.+Impactstatements.mitig.+Impactstatements.unknown.+Impactstatements.unaddressed.
F3 =~ concepts.pub.+concepts.data.+concepts.code.+concepts.infra.+dissChannels.trad.+dissChannels.web.+dissChannels.platf.
F4 =~ targetGroupsGoals.diversity.
F5 =~ scalabilityRating.up.+scalabilityRating.out.+scalabilityRating.deep.
F6 =~ groupsInvolved.policy.+impactTargetGroup.policy.+dissChannels.policy.
F7 =~ transdisciplinaryExp.rate.+groupsInvolved.busi.+impactTargetGroup.pub.+impactTargetGroup.busi.
F8 =~ dissChannels.conf.
F9 =~ contribToSI.rate.
F10 =~ motivation.pheno.+motivation.prob.
'
fit <- cfa(model, data = data.num_questions, )
summary(fit, fit.measures=TRUE, standardized=TRUE)
library(tufte)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
cache.extra = packageVersion("tufte")
)
options(htmltools.dir.version = FALSE)
library(dplyr)
library(plyr)
library(ggplot2)
library(ggbeeswarm)
library(plotly)
library(magrittr)
library(ggplot2)
library(PerformanceAnalytics)
library(psych)
library(corrplot)
library(nFactors)
library(kableExtra)
require(ggiraph)
require(ggiraphExtra)
require(plyr)
library(sjPlot)
library(sjmisc)
library(sjlabelled)
library(tidyverse)
library(gt)
library(grid)
library(ggpubr)
library(psych)
library(GPArotation)
library(car)
library(candisc)
library(jtools)
library(formattable)
# Activate likert and plyr
library(likert)
library(plyr)
library(ggcorrplot)
library(waffle)
library(hrbrthemes)
library(tidyverse)
library(scales)
library(sjPlot)
library(sjmisc)
library(sjlabelled)
library(ggbiplot)
setwd("/home/ubd/Nextcloud/utku_SIVOCS")
# Data Frame
source("./02_analysis/02_static_responses.R")
# colnames of the specific question groups
source("./02_analysis/99_question_groups.R")
rm(list = ls())
library(ggplot2)
library(ggbiplot)
library(psych)
# Data Frame
source("./02_analysis/02_static_responses.R")
# colnames of the specific question groups
source("./02_analysis/99_question_groups.R")
# -------------------- PCA
# Are there completely NA colums?
sum(apply(FUN = sum,
MARGIN = 2,
apply(FUN = is.na,
MARGIN = 2,
data.num_questions)) == nrow(data.num_questions
)
)
pca_model <- prcomp(na.omit(data.num_questions),
scale = TRUE,
center = TRUE)
#plot(pca_model$x[, 1], pca_model$x[, 2])
# How much variation in each component
pca_model.var <- pca_model$sdev^2
pca_model.var_per <- cumsum(pca_model.var)/sum(pca_model.var)
barplot(pca_model.var_per,
main = "Scree Plot")
# A more meaningful visualisation of PCA
pca_model.data <- data.frame(Sample = row.names(pca_model$x),
X = pca_model$x[, 1],
Y = pca_model$x[, 2]
)
ggplot(data = pca_model.data,
aes(x = X, y = Y, label = Sample)) +
geom_text() +
xlab(paste("PC1 - ", round(pca_model.var_per[1], 2), "%", sep = "")) +
ylab(paste("PC2 - ", round(pca_model.var_per[2], 2), "%", sep = "")) +
theme_bw() +
ggtitle("First 2 components")
# Visualisation of all components
ggbiplot(pca_model)
# Most important features
loading_scores <- abs(pca_model$rotation[, 1])
loading_scores.ranked <- sort(loading_scores, decreasing = TRUE)
top_10_features <- loading_scores.ranked[1:10]
top_10_features
# -------------------- Factor Analysis
parallel <- fa.parallel(data.num_questions,
fm = "minres",
fa = 'fa')
cumsum(parallel)
factors <- fa(data.num_questions,
nfactors = 10,
rotate = 'oblimin',
fm = 'minres')
print(factors)
as.vector(rownames(factors$loadings))[(factors$loadings[, "MR10"] > 0.4)]
library(lavaan)
model <- '
F1 =~ groupsInvolved.civsoc.+groupsInvolved.citiz.+groupsInvolved.welfare.+targetGroupsGoals.socneeds.+targetGroupsGoals.socgroups.+targetGroupsGoals.empower.
F2 =~ adoptByPolicy.rate.+Impactstatements.capab.+Impactstatements.emanc.+Impactstatements.understanding.+Impactstatements.mitig.+Impactstatements.unknown.+Impactstatements.unaddressed.
F3 =~ concepts.pub.+concepts.data.+concepts.code.+concepts.infra.+dissChannels.trad.+dissChannels.web.+dissChannels.platf.
F4 =~ targetGroupsGoals.diversity.
F5 =~ scalabilityRating.up.+scalabilityRating.out.+scalabilityRating.deep.
F6 =~ groupsInvolved.policy.+impactTargetGroup.policy.+dissChannels.policy.
F7 =~ transdisciplinaryExp.rate.+groupsInvolved.busi.+impactTargetGroup.pub.+impactTargetGroup.busi.
F8 =~ dissChannels.conf.
F9 =~ contribToSI.rate.
F10 =~ motivation.pheno.+motivation.prob.
'
fit <- cfa(model, data = data.num_questions, )
summary(fit, fit.measures=TRUE, standardized=TRUE)
# colnames of the specific question groups
source("./02_analysis/99_question_groups.R")
# Data Frame
source("./02_analysis/02_static_responses.R")
rm(list = ls())
library(ggplot2)
library(ggbiplot)
library(psych)
# Data Frame
source("./02_analysis/02_static_responses.R")
# colnames of the specific question groups
#source("./02_analysis/99_question_groups.R")
# -------------------- PCA
# Are there completely NA colums?
sum(apply(FUN = sum,
MARGIN = 2,
apply(FUN = is.na,
MARGIN = 2,
data.num_questions)) == nrow(data.num_questions
)
)
pca_model <- prcomp(na.omit(data.num_questions),
scale = TRUE,
center = TRUE)
#plot(pca_model$x[, 1], pca_model$x[, 2])
# How much variation in each component
pca_model.var <- pca_model$sdev^2
pca_model.var_per <- cumsum(pca_model.var)/sum(pca_model.var)
barplot(pca_model.var_per,
main = "Scree Plot")
# A more meaningful visualisation of PCA
pca_model.data <- data.frame(Sample = row.names(pca_model$x),
X = pca_model$x[, 1],
Y = pca_model$x[, 2]
)
ggplot(data = pca_model.data,
aes(x = X, y = Y, label = Sample)) +
geom_text() +
xlab(paste("PC1 - ", round(pca_model.var_per[1], 2), "%", sep = "")) +
ylab(paste("PC2 - ", round(pca_model.var_per[2], 2), "%", sep = "")) +
theme_bw() +
ggtitle("First 2 components")
# Visualisation of all components
ggbiplot(pca_model)
# Most important features
loading_scores <- abs(pca_model$rotation[, 1])
loading_scores.ranked <- sort(loading_scores, decreasing = TRUE)
top_10_features <- loading_scores.ranked[1:10]
top_10_features
# -------------------- Factor Analysis
parallel <- fa.parallel(data.num_questions,
fm = "minres",
fa = 'fa')
cumsum(parallel)
factors <- fa(data.num_questions,
nfactors = 10,
rotate = 'oblimin',
fm = 'minres')
print(factors)
as.vector(rownames(factors$loadings))[(factors$loadings[, "MR10"] > 0.4)]
library(lavaan)
model <- '
F1 =~ groupsInvolved.civsoc.+groupsInvolved.citiz.+groupsInvolved.welfare.+targetGroupsGoals.socneeds.+targetGroupsGoals.socgroups.+targetGroupsGoals.empower.
F2 =~ adoptByPolicy.rate.+Impactstatements.capab.+Impactstatements.emanc.+Impactstatements.understanding.+Impactstatements.mitig.+Impactstatements.unknown.+Impactstatements.unaddressed.
F3 =~ concepts.pub.+concepts.data.+concepts.code.+concepts.infra.+dissChannels.trad.+dissChannels.web.+dissChannels.platf.
F4 =~ targetGroupsGoals.diversity.
F5 =~ scalabilityRating.up.+scalabilityRating.out.+scalabilityRating.deep.
F6 =~ groupsInvolved.policy.+impactTargetGroup.policy.+dissChannels.policy.
F7 =~ transdisciplinaryExp.rate.+groupsInvolved.busi.+impactTargetGroup.pub.+impactTargetGroup.busi.
F8 =~ dissChannels.conf.
F9 =~ contribToSI.rate.
F10 =~ motivation.pheno.+motivation.prob.
'
fit <- cfa(model, data = data.num_questions, )
summary(fit, fit.measures=TRUE, standardized=TRUE)
rm(list = ls())
library(ggplot2)
library(ggbiplot)
library(psych)
# Data Frame
source("./02_analysis/02_static_responses.R")
# colnames of the specific question groups
#source("./02_analysis/99_question_groups.R")
# -------------------- PCA
# Are there completely NA colums?
sum(apply(FUN = sum,
MARGIN = 2,
apply(FUN = is.na,
MARGIN = 2,
data.num_questions)) == nrow(data.num_questions
)
)
pca_model <- prcomp(na.omit(data.num_questions),
scale = TRUE,
center = TRUE)
#plot(pca_model$x[, 1], pca_model$x[, 2])
# How much variation in each component
pca_model.var <- pca_model$sdev^2
pca_model.var_per <- cumsum(pca_model.var)/sum(pca_model.var)
barplot(pca_model.var_per,
main = "Scree Plot")
# A more meaningful visualisation of PCA
pca_model.data <- data.frame(Sample = row.names(pca_model$x),
X = pca_model$x[, 1],
Y = pca_model$x[, 2]
)
ggplot(data = pca_model.data,
aes(x = X, y = Y, label = Sample)) +
geom_text() +
xlab(paste("PC1 - ", round(pca_model.var_per[1], 2), "%", sep = "")) +
ylab(paste("PC2 - ", round(pca_model.var_per[2], 2), "%", sep = "")) +
theme_bw() +
ggtitle("First 2 components")
# Visualisation of all components
ggbiplot(pca_model)
# Most important features
loading_scores <- abs(pca_model$rotation[, 1])
loading_scores.ranked <- sort(loading_scores, decreasing = TRUE)
top_10_features <- loading_scores.ranked[1:10]
top_10_features
# -------------------- Factor Analysis
parallel <- fa.parallel(data.num_questions,
fm = "minres",
fa = 'fa')
cumsum(parallel)
factors <- fa(data.num_questions,
nfactors = 10,
rotate = 'oblimin',
fm = 'minres')
print(factors)
as.vector(rownames(factors$loadings))[(factors$loadings[, "MR10"] > 0.4)]
library(lavaan)
model <- '
F1 =~ groupsInvolved.civsoc.+groupsInvolved.citiz.+groupsInvolved.welfare.+targetGroupsGoals.socneeds.+targetGroupsGoals.socgroups.+targetGroupsGoals.empower.
F2 =~ adoptByPolicy.rate.+Impactstatements.capab.+Impactstatements.emanc.+Impactstatements.understanding.+Impactstatements.mitig.+Impactstatements.unknown.+Impactstatements.unaddressed.
F3 =~ concepts.pub.+concepts.data.+concepts.code.+concepts.infra.+dissChannels.trad.+dissChannels.web.+dissChannels.platf.
F4 =~ targetGroupsGoals.diversity.
F5 =~ scalabilityRating.up.+scalabilityRating.out.+scalabilityRating.deep.
F6 =~ groupsInvolved.policy.+impactTargetGroup.policy.+dissChannels.policy.
F7 =~ transdisciplinaryExp.rate.+groupsInvolved.busi.+impactTargetGroup.pub.+impactTargetGroup.busi.
F8 =~ dissChannels.conf.
F9 =~ contribToSI.rate.
F10 =~ motivation.pheno.+motivation.prob.
'
fit <- cfa(model, data = na.omit(data.num_questions), )
summary(fit, fit.measures=TRUE, standardized=TRUE)
fit <- cfa(model, data = sapply(FUN=scale, na.omit(data.num_questions)), )
summary(fit, fit.measures=TRUE, standardized=TRUE)
na.omit(data.num_questions))
na.omit(data.num_questions)
sapply(FUN=scale, na.omit(data.num_questions))
fit <- cfa(model, data = sapply(FUN=scale, na.omit(data.num_questions)) )
summary(fit, fit.measures=TRUE, standardized=TRUE)
summary(fit, fit.measures=TRUE, standardized=TRUE)
print(factors)
as.vector(rownames(factors$loadings))[(factors$loadings[, "MR10"] > 0.4)]
summary
summary(fit, fit.measures=TRUE, standardized=TRUE)
print(factors)
print(factors[factors > 0.4,])
print(factors)
